{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb69d61f",
   "metadata": {},
   "source": [
    "# LSTM Demo\n",
    "\n",
    "This short demo of using TensorFlow/Keras to produce English words took a tutorial by [Jason Brownlee](https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/) as starting point.\n",
    "\n",
    "We start by importing `tensorflow` functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4aeeb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 11:28:12.944570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-03 11:28:12.944593: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdde43",
   "metadata": {},
   "source": [
    "We now import `numpy`, and `lingpy` (the latter is needed to process the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77b62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lingpy.sequence.sound_classes import get_all_ngrams\n",
    "from lingpy import Wordlist\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b93da",
   "metadata": {},
   "source": [
    "We load the English data from the [WOLD](https://github.com/lexibank/wold) repository using `lingpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc278651",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = Wordlist.from_cldf(\"wold/cldf/cldf-metadata.json\")\n",
    "english = wl.get_list(col=\"English\", entry=\"tokens\", flat=True)\n",
    "data = [\"START \"+\" \".join(x)+\" STOP\" for x in english]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85185ac1",
   "metadata": {},
   "source": [
    "We use the `Tokenizer` class to turn our segmented word forms into numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cac7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer.idx2tok = {idx: w for w, idx in tokenizer.word_index.items()}\n",
    "encoded = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373135c",
   "metadata": {},
   "source": [
    "We now code the sequences for training the model. We split each sequence into all possible n-grams and retain five randomly chosen subsequences with at least two segments (we could train with all n-grams but want to keep training time low for demonstration purposes). We also need to pad the data to the left, in case the sequence is smaller than 20 and truncate it if it is larger, which we do by adding zeros to the left of the numerical sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5095a19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7580\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "for word in encoded:\n",
    "    for i in range(5):\n",
    "        ngram = random.choice([x for x in get_all_ngrams(word) if len(x) > 1])\n",
    "        if ngram:\n",
    "            empty = 21 * [0]\n",
    "            padded_ngram = empty + ngram\n",
    "            sequences.append(padded_ngram[-21:])\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a6f4f",
   "metadata": {},
   "source": [
    "We now split the sequences into our X and y vectors for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0263fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array([row[:20] for row in sequences]), np.array([row[-1] for row in sequences])\n",
    "y = to_categorical(y, num_classes=len(tokenizer.idx2tok)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641db9a2",
   "metadata": {},
   "source": [
    "We now define and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3446dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 11:28:28.484021: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-03 11:28:28.484048: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-03 11:28:28.484068: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (woody): /proc/driver/nvidia/version does not exist\n",
      "2022-02-03 11:28:28.484357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.idx2tok)+1, 10, input_length=20, mask_zero=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(len(tokenizer.idx2tok)+1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9b019",
   "metadata": {},
   "source": [
    "Now we can train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4733b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd03c36d910>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39891650",
   "metadata": {},
   "source": [
    "To retrieve random sequences from this model (English words and pseudowords), we make a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e944d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  1: k ɔː m ə ɹ\n",
      "Word  2: θ ɑː p ə n t\n",
      "Word  3: ɪə l ʌ n\n",
      "Word  4: ð æ f\n",
      "Word  5: b ɔː t\n",
      "Word  6: aɪ t\n",
      "Word  7: ɜː l ə n s\n",
      "Word  8: ʊə w ɛ n t\n",
      "Word  9: r æ\n",
      "Word 10: tʃ ʌ s m\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    current = 19 * [0]\n",
    "    current += [tokenizer.word_index[\"start\"]]\n",
    "    current = np.array([current])\n",
    "    word = []\n",
    "    for j in range(20):\n",
    "        prediction = model.predict(current)\n",
    "        candidate = np.random.choice(len(prediction[0]), p=prediction[0])\n",
    "        segment = tokenizer.idx2tok[candidate]\n",
    "        if segment == \"stop\":\n",
    "            break\n",
    "        word += [segment]\n",
    "        current = list(current[0])+[candidate]\n",
    "        current = np.array([current[-20:]])\n",
    "    print(\"Word {0:2}: {1}\".format(i+1, \" \".join(word)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dd675",
   "metadata": {},
   "source": [
    "This is all, with this short demo, we can see that we can produce existing and potential English sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
